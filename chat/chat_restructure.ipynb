{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disease ID</th>\n",
       "      <th>Disease Name</th>\n",
       "      <th>Affected Plant Species</th>\n",
       "      <th>Symptom Description</th>\n",
       "      <th>Diagnosis Method</th>\n",
       "      <th>Treatment Options</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Powdery Mildew</td>\n",
       "      <td>Cucumber; Zucchini; Grapes</td>\n",
       "      <td>Powdery mildew is characterized by the appeara...</td>\n",
       "      <td>The disease is primarily diagnosed through vis...</td>\n",
       "      <td>To control powdery mildew, sulfur-based fungic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Downy Mildew</td>\n",
       "      <td>Cucumbers; Lettuce; Grapes</td>\n",
       "      <td>Downy mildew manifests as yellowish patches on...</td>\n",
       "      <td>Diagnosis is typically done through careful vi...</td>\n",
       "      <td>Copper-based fungicides or organic treatments ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Leaf Spot</td>\n",
       "      <td>Tomatoes; Potatoes; Lettuce</td>\n",
       "      <td>Leaf spot disease causes small, round lesions ...</td>\n",
       "      <td>Diagnosis is typically based on visual inspect...</td>\n",
       "      <td>Use fungicides or bactericides depending on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Root Rot</td>\n",
       "      <td>Tomatoes; Lettuce; Cucumbers</td>\n",
       "      <td>Root rot leads to wilting and yellowing of the...</td>\n",
       "      <td>Diagnosed by examining the roots for signs of ...</td>\n",
       "      <td>Improve soil drainage and avoid over-watering....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Late Blight</td>\n",
       "      <td>Potatoes; Tomatoes</td>\n",
       "      <td>Late blight results in dark, water-soaked lesi...</td>\n",
       "      <td>Diagnosis is typically confirmed by observing ...</td>\n",
       "      <td>Use fungicides containing copper or metalaxyl ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Disease ID    Disease Name        Affected Plant Species  \\\n",
       "0           1  Powdery Mildew    Cucumber; Zucchini; Grapes   \n",
       "1           2    Downy Mildew    Cucumbers; Lettuce; Grapes   \n",
       "2           3       Leaf Spot   Tomatoes; Potatoes; Lettuce   \n",
       "3           4        Root Rot  Tomatoes; Lettuce; Cucumbers   \n",
       "4           5     Late Blight            Potatoes; Tomatoes   \n",
       "\n",
       "                                 Symptom Description  \\\n",
       "0  Powdery mildew is characterized by the appeara...   \n",
       "1  Downy mildew manifests as yellowish patches on...   \n",
       "2  Leaf spot disease causes small, round lesions ...   \n",
       "3  Root rot leads to wilting and yellowing of the...   \n",
       "4  Late blight results in dark, water-soaked lesi...   \n",
       "\n",
       "                                    Diagnosis Method  \\\n",
       "0  The disease is primarily diagnosed through vis...   \n",
       "1  Diagnosis is typically done through careful vi...   \n",
       "2  Diagnosis is typically based on visual inspect...   \n",
       "3  Diagnosed by examining the roots for signs of ...   \n",
       "4  Diagnosis is typically confirmed by observing ...   \n",
       "\n",
       "                                   Treatment Options  \n",
       "0  To control powdery mildew, sulfur-based fungic...  \n",
       "1  Copper-based fungicides or organic treatments ...  \n",
       "2  Use fungicides or bactericides depending on th...  \n",
       "3  Improve soil drainage and avoid over-watering....  \n",
       "4  Use fungicides containing copper or metalaxyl ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/Advisor_data/final_descriptive.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_species(species):\n",
    "    # Split the string into a list of words by ';' and remove extra spaces\n",
    "    species_list = [s.strip() for s in species.split(';')]\n",
    "    \n",
    "    # If there's only one item, return it in singular form\n",
    "    if len(species_list) == 1:\n",
    "        return f\"It affects the plants such as {species_list[0]}\"\n",
    "    \n",
    "    # If multiple items, join them with commas and \"and\" before the last item\n",
    "    species_str = ', '.join(species_list[:-1]) + f\" and {species_list[-1]}\"\n",
    "    return f\"It affects the plants such as {species_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1556\n",
      "CSV has been successfully created\n"
     ]
    }
   ],
   "source": [
    "json_data = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    input_text = f\"Which plants are affected by {row['Disease Name']}?\"\n",
    "    response = format_species(row['Affected Plant Species'])\n",
    "    \n",
    "    json_data.append({\n",
    "        \"input\": input_text,\n",
    "        \"response\": response\n",
    "    })\n",
    "\n",
    "    input_text = f\"What are the diagnosis methods for {row['Disease Name']}?\"\n",
    "    response = row['Diagnosis Method']\n",
    "\n",
    "    json_data.append({\n",
    "        'input': input_text,\n",
    "        'response':response\n",
    "        \n",
    "    })\n",
    "    \n",
    "    input_text = f\"What are the symptons of {row['Disease Name']}?\"\n",
    "    response = row['Symptom Description']\n",
    "\n",
    "    json_data.append({\n",
    "        'input': input_text,\n",
    "        'response':response\n",
    "        \n",
    "    })\n",
    "    \n",
    "    input_text = f\"What are the treatment options for {row['Disease Name']}?\"\n",
    "    response = row['Treatment Options']\n",
    "    \n",
    "    json_data.append({\n",
    "        \"input\": input_text,\n",
    "        \"response\": response\n",
    "    })\n",
    "\n",
    "print(len(json_data))\n",
    "\n",
    "df_json = pd.DataFrame(json_data)\n",
    "\n",
    "df_json.to_csv('descriptive_csv.csv',index=False)\n",
    "\n",
    "print('CSV has been successfully created')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which plants are affected by Powdery Mildew?</td>\n",
       "      <td>It affects the plants such as Cucumber, Zucchi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the diagnosis methods for Powdery Mil...</td>\n",
       "      <td>The disease is primarily diagnosed through vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the symptons of Powdery Mildew?</td>\n",
       "      <td>Powdery mildew is characterized by the appeara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the treatment options for Powdery Mil...</td>\n",
       "      <td>To control powdery mildew, sulfur-based fungic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which plants are affected by Downy Mildew?</td>\n",
       "      <td>It affects the plants such as Cucumbers, Lettu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0       Which plants are affected by Powdery Mildew?   \n",
       "1  What are the diagnosis methods for Powdery Mil...   \n",
       "2           What are the symptons of Powdery Mildew?   \n",
       "3  What are the treatment options for Powdery Mil...   \n",
       "4         Which plants are affected by Downy Mildew?   \n",
       "\n",
       "                                            response  \n",
       "0  It affects the plants such as Cucumber, Zucchi...  \n",
       "1  The disease is primarily diagnosed through vis...  \n",
       "2  Powdery mildew is characterized by the appeara...  \n",
       "3  To control powdery mildew, sulfur-based fungic...  \n",
       "4  It affects the plants such as Cucumbers, Lettu...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_df = pd.read_csv('./descriptive_csv.csv')\n",
    "r_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The disease is primarily diagnosed through visual inspection, where the white, powdery fungal growth is easily identifiable. Microscopic examination can confirm the presence of fungal spores. In some cases, laboratory culturing techniques may be used to isolate and identify the specific pathogen responsible for the infection.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_df['response'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input       0\n",
       "response    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "r_df['input_tokens'] = r_df['input'].apply(tokenize)\n",
    "r_df['response_tokens'] = r_df['response'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "all_tokens= [token for tokens in r_df['input_tokens'] for token in tokens] + \\\n",
    "            [token for tokens in r_df['response_tokens'] for token in tokens]\n",
    "\n",
    "vocab = Counter(all_tokens)\n",
    "vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "# Create word-to-word and index-to-word mappings\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_indices(tokens, word2idx):\n",
    "    return [word2idx[token] for token in tokens]\n",
    "\n",
    "r_df['input_indices'] = r_df['input_tokens'].apply(lambda x: tokens_to_indices(x, word2idx))\n",
    "r_df['response_indices'] = r_df['response_tokens'].apply(lambda x: tokens_to_indices(x, word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Pad sequences\n",
    "input_sequences = [torch.tensor(seq) for seq in r_df['input_indices']]\n",
    "response_sequences = [torch.tensor(seq) for seq in r_df['response_indices']]\n",
    "\n",
    "input_padded = pad_sequence(input_sequences, batch_first=True, padding_value=0)\n",
    "response_padded = pad_sequence(response_sequences, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq, target_seq=None):\n",
    "        # Embed input sequence\n",
    "        embedded = self.embedding(input_seq)\n",
    "\n",
    "        # Encode input sequence\n",
    "        _, hidden = self.encoder(embedded)\n",
    "\n",
    "        # Decode sequence\n",
    "        if target_seq is not None:\n",
    "            embedded_target = self.embedding(target_seq)\n",
    "            output, _ = self.decoder(embedded_target, hidden)\n",
    "            output = self.fc(output)\n",
    "            return output\n",
    "        else:\n",
    "            # For inference, generate tokens one by one\n",
    "            output_tokens = []\n",
    "            current_token = torch.tensor([[word2idx['<start>']]], device=input_seq.device)\n",
    "            for _ in range(50):  # Max length of response\n",
    "                embedded_token = self.embedding(current_token)\n",
    "                output, hidden = self.decoder(embedded_token, hidden)\n",
    "                output = self.fc(output)\n",
    "                predicted_token = output.argmax(dim=-1)\n",
    "                output_tokens.append(predicted_token.item())\n",
    "                current_token = predicted_token.unsqueeze(0)\n",
    "                if predicted_token.item() == word2idx['<end>']:\n",
    "                    break\n",
    "            return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create dataset\n",
    "dataset = TensorDataset(input_padded, response_padded)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "\n",
    "# Initialize model\n",
    "model = Seq2Seq(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 5.399273278761883\n",
      "Epoch 2/10, Loss: 3.3250484709837\n",
      "Epoch 3/10, Loss: 2.464319744888617\n",
      "Epoch 4/10, Loss: 1.9902333872658866\n",
      "Epoch 5/10, Loss: 1.6919951755173352\n",
      "Epoch 6/10, Loss: 1.456469097915961\n",
      "Epoch 7/10, Loss: 1.283072946022968\n",
      "Epoch 8/10, Loss: 1.1536807563840126\n",
      "Epoch 9/10, Loss: 1.0410142991007592\n",
      "Epoch 10/10, Loss: 0.9455777844604181\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for input_batch, target_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_batch, target_batch[:, :-1])\n",
    "        loss = criterion(output.view(-1, vocab_size), target_batch[:, 1:].reshape(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<start>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     14\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhich plants are affected by Powdery Mildew?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m, in \u001b[0;36mgenerate_response\u001b[1;34m(model, input_text)\u001b[0m\n\u001b[0;32m      5\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([indices], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 8\u001b[0m     output_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([idx2word[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m output_tokens])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mD:\\AI and ML\\plant_disease_classifier_application\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\AI and ML\\plant_disease_classifier_application\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[21], line 27\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[1;34m(self, input_seq, target_seq)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# For inference, generate tokens one by one\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     output_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 27\u001b[0m     current_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[43mword2idx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<start>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m]], device\u001b[38;5;241m=\u001b[39minput_seq\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# Max length of response\u001b[39;00m\n\u001b[0;32m     29\u001b[0m         embedded_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(current_token)\n",
      "\u001b[1;31mKeyError\u001b[0m: '<start>'"
     ]
    }
   ],
   "source": [
    "def generate_response(model, input_text):\n",
    "    model.eval()\n",
    "    tokens = tokenize(input_text)\n",
    "    indices = tokens_to_indices(tokens, word2idx)\n",
    "    input_seq = torch.tensor([indices], dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model(input_seq)\n",
    "\n",
    "    response = ' '.join([idx2word[idx] for idx in output_tokens])\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Which plants are affected by Powdery Mildew?\"\n",
    "response = generate_response(model, input_text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline, set_seed\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments,Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which plants are affected by Powdery Mildew?</td>\n",
       "      <td>It affects the plants such as Cucumber, Zucchi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the diagnosis methods for Powdery Mil...</td>\n",
       "      <td>The disease is primarily diagnosed through vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the symptons of Powdery Mildew?</td>\n",
       "      <td>Powdery mildew is characterized by the appeara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the treatment options for Powdery Mil...</td>\n",
       "      <td>To control powdery mildew, sulfur-based fungic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which plants are affected by Downy Mildew?</td>\n",
       "      <td>It affects the plants such as Cucumbers, Lettu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0       Which plants are affected by Powdery Mildew?   \n",
       "1  What are the diagnosis methods for Powdery Mil...   \n",
       "2           What are the symptons of Powdery Mildew?   \n",
       "3  What are the treatment options for Powdery Mil...   \n",
       "4         Which plants are affected by Downy Mildew?   \n",
       "\n",
       "                                            response  \n",
       "0  It affects the plants such as Cucumber, Zucchi...  \n",
       "1  The disease is primarily diagnosed through vis...  \n",
       "2  Powdery mildew is characterized by the appeara...  \n",
       "3  To control powdery mildew, sulfur-based fungic...  \n",
       "4  It affects the plants such as Cucumbers, Lettu...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./descriptive_csv.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = train_test_split(df,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas DataFrame to Hugging Face dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data[['input', 'response']])\n",
    "test_dataset = Dataset.from_pandas(test_data[['input', 'response']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'response', '__index_level_0__'],\n",
       "    num_rows: 1244\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'  \n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# def tokenizer_function(examples):\n",
    "#     return tokenizer(examples['input'], truncation = True, padding='max_length', max_length=512)\n",
    "\n",
    "# train_dataset = train_dataset.map(tokenizer_function, batched=True)\n",
    "# test_dataset = test_dataset.map(tokenizer_function, batched=True)\n",
    "# Tokenize the dataset\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Combine input and response into a single string for training\n",
    "    combined_text = [f\"{inp} {tokenizer.eos_token} {resp}\" for inp, resp in zip(examples['input'], examples['response'])]\n",
    "    tokenized = tokenizer(combined_text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # Labels are the same as input_ids for next-token prediction\n",
    "    print(tokenized)\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modify the inputs and labels for language modeling (shift labels for next-token prediction)\n",
    "# def shift_labels(batch):\n",
    "#     # Shift the labels by one token\n",
    "#     batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
    "#     batch[\"labels\"] = [ids[1:] + [tokenizer.pad_token_id] for ids in batch[\"labels\"]]  # Shift labels\n",
    "#     return batch\n",
    "\n",
    "# train_dataset = train_dataset.map(shift_labels, batched=True)\n",
    "# test_dataset = test_dataset.map(shift_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'response', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1244\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./gpt2_chat_model/',\n",
    "    eval_strategy= 'epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_eval_batch_size= 4,\n",
    "    per_gpu_eval_batch_size= 4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    push_to_hub= False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset= train_dataset,\n",
    "    eval_dataset= test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 01:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.466400</td>\n",
       "      <td>0.419637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.382600</td>\n",
       "      <td>0.361668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.282900</td>\n",
       "      <td>0.344676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=468, training_loss=0.4826259032273904, metrics={'train_runtime': 111.3998, 'train_samples_per_second': 33.501, 'train_steps_per_second': 4.201, 'total_flos': 243785465856000.0, 'train_loss': 0.4826259032273904, 'epoch': 3.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2_finetuned_model\\\\tokenizer_config.json',\n",
       " './gpt2_finetuned_model\\\\special_tokens_map.json',\n",
       " './gpt2_finetuned_model\\\\vocab.json',\n",
       " './gpt2_finetuned_model\\\\merges.txt',\n",
       " './gpt2_finetuned_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model and tokenizer\n",
    "trainer.save_model('./gpt2_finetuned_model')  # Save model weights and config\n",
    "tokenizer.save_pretrained('./gpt2_finetuned_model')  # Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.34467554092407227, 'eval_runtime': 2.6356, 'eval_samples_per_second': 118.379, 'eval_steps_per_second': 29.595, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "results = trainer.evaluate(test_dataset)\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "made through visual inspection of the yellowing and mold growth. Laboratory tests can confirm the presence of the fungal pathogen.\n"
     ]
    }
   ],
   "source": [
    "def chat_with_model(model, tokenizer, input_text, max_length=100, temperature=0.7, top_k=50, top_p=0.95):\n",
    "    # Append a separator token to the input text\n",
    "    input_text = f\"{input_text} {tokenizer.eos_token}\"\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Ensure the model is on the correct device (GPU or CPU)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Generate a response\n",
    "    output = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],  # Pass attention_mask\n",
    "        max_length=max_length,                   # Increase max_length for longer responses\n",
    "        num_return_sequences=1,\n",
    "        temperature=temperature,  # Adjust temperature for more varied responses\n",
    "        top_k=top_k,              # Limit sampling to top-k tokens\n",
    "        top_p=top_p,              # Use nucleus sampling\n",
    "        do_sample=True,           # Enable sampling (instead of greedy decoding)\n",
    "        pad_token_id=tokenizer.eos_token_id  # Set pad_token_id to eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode the output text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove the input text from the generated response\n",
    "    generated_text = generated_text[len(input_text):].strip()\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "input_text = \"What are the diagnosis methods for Powdery Mildew?\"\n",
    "response = chat_with_model(model, tokenizer, input_text)\n",
    "print(\"Generated Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
